---
title: "Summary Report of Pairings 1 2 5 12 And 16"
author: "Group 6"
date: "4/7/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this project, we are implementing 5 algorithms:

1. A1+D1 Propensity Matching with Mahalanobis distance measure
2. A1+D2+P1 Propensity Matching with propensity score distance measure + logistic regression
3. A1+D3+P1 Propensity Matching with linear Propensity score distance measure + logistic regression
4. A3+P1 Doubly Robust Estimation + logistic regression
5. A5+P1 Stratification + logistic regression


## Setting up environment

```{r install package, echo=FALSE, results = FALSE, include=FALSE}
if(!require(MatchIt)) install.packages("MatchIt", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lmtest)) install.packages("lmtest", repos = "http://cran.us.r-project.org")
if(!require(sandwich)) install.packages("sandwich", repos = "http://cran.us.r-project.org")
if(!require(optmatch)) install.packages("optmatch", repos = "http://cran.us.r-project.org")
if(!require(broom)) install.packages("broom", repos = "http://cran.us.r-project.org")
```

```{r library}
library(MatchIt)
library(dplyr)
library(ggplot2)
library(lmtest) 
library(sandwich) 
library(optmatch)
library(broom)
```

## Pre-Analysis
```{r Difference in Means Pre-Analysis}
# Reading in the datasets as R Dataframes
highDim_data = read.csv("../data/highDim_dataset.csv")
lowDim_data = read.csv("../data/lowDim_dataset.csv")

highDim_data %>% 
  group_by(A) %>% 
  summarise(n=n(),
            mean_outcome=mean(Y),
            std_error = sd(Y) / sqrt(n), .groups = "drop")

with(highDim_data, t.test(Y ~ A))

lowDim_data %>% 
  group_by(A) %>% 
  summarise(n=n(),
            mean_outcome=mean(Y),
            std_error = sd(Y) / sqrt(n), .groups = "drop")

with(lowDim_data, t.test(Y ~ A))

true_low_ATE<-2.0901
true_high_ATE<- -54.8558
```



#### Use Logistic regression to get the propensity score
```{r Propensity Score Estimation}
# High dimension dataset
hd_time = system.time({ps_hd_est <- glm(A ~ . -Y,
                 family=binomial(), 
                 data=highDim_data)

ps_hd_df <- data.frame(prop_score = predict(ps_hd_est, type = "response"),
                       treatment = ps_hd_est$model$A,
                       Y = ps_hd_est$model$Y)
})
ps_hd_time = round(as.numeric(hd_time[3]),3)

ps_hd_df %>%
  group_by(treatment) %>% 
  summarise(mean_prop_score=mean(prop_score), .groups = "drop")
  
labs <- paste("Actual treatment indicator:", c("Treated", "Control"))
ps_hd_df %>%
  mutate(treatment = ifelse(treatment == 1, labs[1], labs[2])) %>%
  ggplot(aes(x = prop_score), binwidth = 30) +
  geom_histogram(color = "white", bins=30) +
  facet_wrap(~treatment) +
  xlab("Probability of being exposed") +
  theme_bw()

# Low dimension dataset
ld_time = system.time({ps_ld_est <- glm(A ~ . -Y,
                 family=binomial(), 
                 data=lowDim_data)

ps_ld_df <- data.frame(prop_score = predict(ps_ld_est, type = "response"),
                       treatment = ps_ld_est$model$A,
                       Y = ps_ld_est$model$Y)
})
ps_ld_time = round(as.numeric(ld_time[3]),3)

ps_ld_df %>%
  group_by(treatment) %>% 
  summarise(mean_prop_score=mean(prop_score), .groups = "drop")
  
labs <- paste("Actual treatment indicator:", c("Treated", "Control"))
ps_ld_df %>%
  mutate(treatment = ifelse(treatment == 1, labs[1], labs[2])) %>%
  ggplot(aes(x = prop_score)) +
  geom_histogram(color = "white", bins=30) +
  facet_wrap(~treatment) +
  xlab("Probability of being exposed") +
  theme_bw()
```


# Causal Inference Methods

## Pairing 1

>Algorithm: `Propensity Matching`

>Distance Measure: `Mahalanobis`

>Propensity Score Estimation: `NA`


### Introduction of the algorithm

* Propensity Matching motivations:

key assumption: If a control and treated individual are identical before treatment, the probability of the outcome variable is equal where there is no treatment applied ($Y_0$).

Matching on variables $X$ thus ensures independence between treatment and $Y_0$ Matching on $X$ is often infeasible especially where $X$ is high dimensional. Instead, we use alternate distance measures to match on in order to circumvent this obstacle.

key parameters: 

Distance Measure: Measure of similarity between 2 individuals
Matching Method: How matching is conducted between individuals

* When estimating the ATE, either subclassification or full matching can be used. Full matching can be more effective because it optimizes a balance criterion, often leading to better balance. With full matching, it’s also possible to exact match on some variables and match using the Mahalanobis distance, eliminating the need to estimate propensity scores. However, for large datasets, full matching may not be possible, in which case subclassification is a faster solution. 


* The Mahalanobis distance is defined as:

$$D_ij=(X_i-X_j)^{T} \Sigma^{-1}(X_i-X_j)$$
Where $\Sigma$ is the variance covariance matrix of X.

* Mahalanobis Distance does not require propensity score estimation and performs best with continuous variables.

###### implementation 1 code
```{r ATE(average treatment effect), running time of pairing 1}
mahalanobis.ate <- function(data){
  varnum=dim(data)[2]-2
  xnam <- paste0("V", 1:varnum)
  start_time <- Sys.time()
  match_full<-matchit(as.formula(paste("A ~ ", paste(xnam, collapse= "+"))),
                      data=data,method="full", distance="mahalanobis",
                      link = "logit", estimand = "ATE")
  data.fullMatching <- match.data(match_full)
  x = data.fullMatching %>% 
    group_by(subclass,A) %>% 
    summarise(mean_y = mean(Y), .groups = 'drop')
  group_ate = x %>% 
    group_by(subclass) %>% 
    summarise(treat_eff = mean_y[A == 1] - mean_y[A == 0], .groups = 'drop')
  group_n = data.fullMatching %>% group_by(subclass) %>% count()
  ate = sum(group_ate$treat_eff*group_n$n/nrow(data))
  end_time <- Sys.time()
  return(list(ATE=ate,running_time = end_time - start_time))
}
```

###### implementation 2 code
```{r Pairing 1 Implementing Matching High Dim}
# Algorithm: Propensity Matching
# Distance Measure: Mahalanobis
# Propensity Score Estimation: NA
# Matching performed on low dimension dataset
covs = colnames(highDim_data)[-2:-1]
pair1_highdim_time = system.time({
  if (length(na.omit(highDim_data)) != length(highDim_data)) {
    print('There are null values in the dataset')
    break
    } else {
    pair_1 <- matchit(A ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 + V32 + V33 + V34 + V35 + V36 + V37 + V38 + V39 + V40 + V41 + V42 + V43 + V44 + V45 + V46 + V47 + V48 + V49 + V50 + V51 + V52 + V53 + V54 + V55 + V56 + V57 + V58 + V59 + V60 + V61 + V62 + V63 + V64 + V65 + V66 + V67 + V68 + V69 + V70 + V71 + V72 + V73 + V74 + V75 + V76 + V77 + V78 + V79 + V80 + V81 + V82 + V83 + V84 + V85 + V86 + V87 + V88 + V89 + V90 + V91 + V92 + V93 + V94 + V95 + V96 + V97 + V98 + V99 + V100 + V101 + V102 + V103 + V104 + V105 + V106 + V107 + V108 + V109 + V110 + V111 + V112 + V113 + V114 + V115 + V116 + V117 + V118 + V119 + V120 + V121 + V122 + V123 + V124 + V125 + V126 + V127 + V128 + V129 + V130 + V131 + V132 + V133 + V134 + V135 + V136 + V137 + V138 + V139 + V140 + V141 + V142 + V143 + V144 + V145 + V146 + V147 + V148 + V149 + V150 + V151 + V152 + V153 + V154 + V155 + V156 + V157 + V158 + V159 + V160 + V161 + V162 + V163 + V164 + V165 + V166 + V167 + V168 + V169 + V170 + V171 + V172 + V173 + V174 + V175 + V176 + V177 + V178 + V179 + V180 + V181 + V182 + V183 + V184 + V185,
                      data=highDim_data,
                      method="full",
                      distance="mahalanobis",
                      estimand = "ATE")
  }  
# Preparing Data Frame based on Pairing 1 Matches High Dim
mpair_1 = match.data(pair_1)
#Estimating Treatment Effects using Pairing 1 Matches High Dim
mpair1_fit <- lm(Y ~ . -Y -weights -subclass,
                 data = mpair_1,
                 weights = weights)
pair1_highdim_ATE = round(as.numeric(mpair1_fit$coefficients['A']),4)
})
pair1_hd_time <- round(as.numeric(pair1_highdim_time[3]),3)
```

```{r Pairing 1 Implementing Matching Low Dim}
# Algorithm: Propensity Matching
# Distance Measure: Mahalanobis
# Propensity Score Estimation: NA
# Matching performed on low dimension dataset
covs = colnames(lowDim_data)[-2:-1]
pair1_lowdim_time = system.time({
  if (length(na.omit(lowDim_data)) != length(lowDim_data)) {
    print('There are null values in the dataset')
    break
    } else {
    pair_1 <- matchit(A ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22,
                      data=lowDim_data,
                      method="full",
                      distance="mahalanobis",
                      estimand = "ATE")
  }  
# Preparing Data Frame based on Pairing 1 Matches Low Dim
mpair_1 = match.data(pair_1)
# Estimating Treatment Effects using Pairing 1 Matches Low Dim
mpair1_fit <- lm(Y ~ . -Y -weights -subclass,
                 data = mpair_1,
                 weights = weights)
pair1_lowdim_ATE = round(as.numeric(mpair1_fit$coefficients['A']),4)
})
pair1_ld_time <- round(as.numeric(pair1_lowdim_time[3]),3)
```


#### High Dim data

```{r highDim for pairing1 method2, echo=FALSE}
pair1high = mahalanobis.ate(highDim_data)

cat("ATE for implementation 1 is:", pair1high$ATE, ".\n")
cat("ATE error for implementation 1 is:", true_high_ATE-pair1high$ATE, ".\n")
cat("Processing time for implementation 1 is" , pair1high$running_time, "seconds.\n")
cat("\nATE for implementation 2 is:", pair1_highdim_ATE, ".\n")
cat("ATE error for implementation 2 is:", true_high_ATE-pair1_highdim_ATE, ".\n")
cat("Processing time for implementation 2 is" , pair1_hd_time, "seconds.\n")
```

#### Low Dim data

```{r lowDim for pairing2 method2, echo=FALSE}
pair1low = mahalanobis.ate(lowDim_data)

cat("ATE for implementation 1 is:", pair1low$ATE, ".\n")
cat("ATE error for implementation 1 is:", true_low_ATE-pair1low$ATE, ".\n")
cat("Processing time for implementation 1 is" ,pair1low$running_time, "seconds.\n")
cat("\nATE for implementation 2 is:", pair1_lowdim_ATE, ".\n")
cat("ATE error for implementation 2 is:", true_low_ATE-pair1_lowdim_ATE, ".\n")
cat("Processing time for implementation 2 is" , pair1_ld_time, "seconds.\n")
```

***

## Pairing 2

>Algorithm: `Propensity Matching`

>Distance Measure: `Propensity Score`

>Propensity Score Estimation: `Logistic Regression`

### Introduction of the algorithm

* The distance measure is defined as:
$$ D_{ij} = |e_i-e_j|$$

where $e_k$ is the propensity score for individual k.

* Propensity score definition:
$$p(\textbf{X})=P(exposed | \textbf{X})$$
It is the probability of an individual being exposed (treated) given individual-specific characteristics.


###### implementation 1 code
```{r ATE(average treatment effect), running time of pairing 2}
Propensity.Score.ate <- function(data, methods, link){
  start_time <- Sys.time()
  match_full<-matchit(A ~ .-Y,data=data,method=methods,distance="glm",link = link, estimand = "ATE")
  data.fullMatching <- match.data(match_full)
  x = data.fullMatching %>% group_by(subclass,A) %>% summarise(mean_y = mean(Y), .groups = 'drop')
  group_ate = x %>% group_by(subclass) %>% summarise(treat_eff = mean_y[A == 1] - mean_y[A == 0], .groups = 'drop')
  group_n = data.fullMatching %>% group_by(subclass) %>% count()
  ate = sum(group_ate$treat_eff*group_n$n/nrow(data))
  end_time <- Sys.time()
  return(list(ATE=round(ate,4),running_time = round(end_time - start_time, 3)))
}
```


###### implementation 2 code
```{r Pairing 2 Implementing Matching Algo}
# Algorithm: Propensity Matching
# Distance Measure: Propensity Score
# Propensity Score Estimation: Logistic Regression
pair2 <- function(data, method) {
  pair2_time = system.time({
    if (length(na.omit(data)) != length(data)) {
      print('There are null values in the dataset')
      break
      } 
    else {
      pair_2 <- matchit(A ~ .-Y,
                        data=data,
                        method=method,
                        distance="glm",
                        link="logit",
                        estimand = "ATE")
    }  
    # Preparing Data Frame based on Pairing 2 Matches
    mpair_2 = match.data(pair_2)
    # Estimating Treatment Effects using Pairing 2 Matches
    mpair2_fit <- lm(Y ~ . -Y -weights -subclass -distance,
                     data = mpair_2,
                     weights = weights)
    pair2_ATE = round(as.numeric(mpair2_fit$coefficients['A']),4)
    })
  pair2_time <- round(as.numeric(pair2_time[3]),3)
  return(list(ATE=pair2_ATE, running_time = pair2_time))
}
```

#### optimal full matching
##### High Dim data
```{r highDim for pairing 2 full}
pair2fhigh = Propensity.Score.ate(highDim_data, methods="full", link="logit")
pair2fhigh_alt = pair2(highDim_data, method="full")
```


```{r pairing2 result high dim, echo=FALSE}
cat("ATE for implementation 1 is:", pair2fhigh$ATE, ".\n")
cat("ATE error for implementation 1 is:", true_high_ATE-pair2fhigh$ATE, ".\n")
cat("Processing time for implementation 1 is" ,pair2fhigh$running_time, "seconds.\n")
cat("\nATE for implementation 2 is:", pair2fhigh_alt$ATE, ".\n")
cat("ATE error for implementation 2 is:", true_high_ATE-pair2fhigh_alt$ATE, ".\n")
cat("Processing time for implementation 2 is" ,pair2fhigh_alt$running_time, "seconds.\n")
cat("Processing time for calculating the propensity score is", ps_hd_time, "seconds.\n")
```

##### Low Dim data
```{r lowDim for pairing 2 full}
pair2flow = Propensity.Score.ate(lowDim_data, methods="full", link="logit")
pair2flow_alt = pair2(lowDim_data, method="full")
```

```{r pairing2 result lwo dim, echo=FALSE}
cat("ATE for implementation 1 is:", pair2flow$ATE, ".\n")
cat("ATE error for implementation 1 is:", true_low_ATE-pair2flow$ATE, ".\n")
cat("Processing time for implementation 1 is" ,pair2flow$running_time, "seconds.\n")
cat("\nATE for implementation 2 is:", pair2flow_alt$ATE, ".\n")
cat("ATE error for implementation 2 is:", true_low_ATE-pair2flow_alt$ATE, ".\n")
cat("Processing time for implementation 2 is" ,pair2flow_alt$running_time, "seconds.\n")
cat("\nProcessing time for calculating the propensity score is", ps_ld_time, "seconds.\n")
```

#### subclassification
##### High Dim data
```{r highDim for pairing 2 sub}
pair2shigh = Propensity.Score.ate(highDim_data, methods="subclass", link="logit")
pair2shigh_alt = pair2(highDim_data, method="subclass")
```

```{r sub result high dim, echo=FALSE}
cat("ATE for implementation 1 is:", pair2shigh$ATE, ".\n")
cat("ATE error for implementation 1 is:", true_high_ATE-pair2shigh$ATE, ".\n")
cat("Processing time for implementation 1 is" ,pair2shigh$running_time, "seconds.\n")
cat("\nATE for implementation 2 is:", pair2shigh_alt$ATE, ".\n")
cat("ATE error for implementation 2 is:", true_high_ATE-pair2shigh_alt$ATE, ".\n")
cat("Processing time for implementation 2 is" ,pair2shigh_alt$running_time, "seconds.\n")
cat("\nProcessing time for calculating the propensity score is", ps_hd_time, "seconds.\n")
```

##### Low Dim data
```{r lowDim for pairing 2 sub}
pair2slow = Propensity.Score.ate(lowDim_data, methods="subclass", link="logit")
pair2slow_alt = pair2(lowDim_data, method="subclass")
```

```{r sub result low dim, echo=FALSE}
cat("ATE for implementation 1 is:", pair2slow$ATE, ".\n")
cat("ATE error for implementation 1 is:", true_low_ATE-pair2slow$ATE, ".\n")
cat("Processing time for implementation 1 is" ,pair2slow$running_time, "seconds.\n")
cat("\nATE for implementation 2 is:", pair2slow_alt$ATE, ".\n")
cat("ATE error for implementation 2 is:", true_low_ATE-pair2slow_alt$ATE, ".\n")
cat("Processing time for implementation 2 is" ,pair2slow_alt$running_time, "seconds.\n")
cat("\nProcessing time for calculating the propensity score is", ps_ld_time, "seconds.\n")
```

***

## Pairing 5

>Algorithm: `Propensity Matching`

>Distance Measure: `Linear Propensity Score`

>Propensity Score Estimation: `Logistic Regression`

### Introduction of the algorithm
The distance of Propensity Score is defined as:
$$D_{ij}=|e_i-e_j|$$
Obtained by applying the logit function on the Propensity Scores.
Matching on the linear propensity score can be particularly effective in terms of reducing bias.

Linear propensity score matching is same with propensity score to entail forming matched sets of treated and untreated subjects who share a similar value of the propensity score. Once a matched sample has been formed, the treatment effect can be estimated by directly comparing outcomes between treated and untreated subjects in the matched sample. Once the effect of treatment has been estimated in the propensity score matched sample, the variance of the estimated treatment effect and its statistical significance can be estimated. 

After the matched sets are obtained, Linear Propensity Score asks to calculate a “subclass effects” for each matched set/subclass, and then estimate overall ATE by an weighted average of the subclass effects where weights would be the number of individuals in each subclass.


Linear Propensity score performed well for low dimensional dataset as explained above for standard propensity score.

Linear Propensity Score didn't perform as well for high dimension as low dimensional dataset for the same reason discussed above for standard Propensity Score.

### optimal full matching
```{r ATE(average treatment effect), running time of pairing 5}
pair5fhigh = Propensity.Score.ate(highDim_data, methods="full", link="linear.logit")
pair5flow = Propensity.Score.ate(lowDim_data, methods="full", link="linear.logit")
```

##### High Dim data
```{r highDim data, echo=FALSE}
cat("ATE for high dimensional data is:", pair5fhigh$ATE, ".\n")
cat("ATE error for high dimensional data is:", true_high_ATE-pair5fhigh$ATE, ".\n")
cat("Processing time for calculating the propensity score is", ps_hd_time, "seconds.\n")
cat("Processing time for pairing 5 with full matching is" ,pair5fhigh$running_time, "seconds.\n")
```

##### Low Dim data
```{r lowDim data, echo=FALSE}
cat("ATE for low dimensional data is:", pair5flow$ATE, ".\n")
cat("ATE error for low dimensional data is:", true_low_ATE-pair5flow$ATE, ".\n")
cat("Processing time for calculating the propensity score is", ps_ld_time, "seconds.\n")
cat("Processing time for pairing 5 with full matching is" ,pair5flow$running_time, "seconds.\n")
```

#### subclassification
```{r subclass, running time of pairing 5}
pair5shigh = Propensity.Score.ate(highDim_data, methods="subclass", link="linear.logit")
pair5slow = Propensity.Score.ate(lowDim_data, methods="subclass", link="linear.logit")
```

##### High Dim data
```{r highDim data for subclass, echo=FALSE}
cat("ATE for high dimensional data is:", pair5shigh$ATE, ".\n")
cat("ATE error for high dimensional data is:", true_high_ATE-pair5shigh$ATE, ".\n")
cat("Processing time for calculating the propensity score is", ps_hd_time, "seconds.\n")
cat("Processing time for runing pairing 5 is" ,pair5shigh$running_time, "seconds.\n")
```

##### Low Dim data
```{r lowDim data for subclass, echo=FALSE}
cat("ATE for low dimensional data is:", pair5slow$ATE, ".\n")
cat("ATE error for low dimensional data is:", true_low_ATE-pair5slow$ATE, ".\n")
cat("Processing time for calculating the propensity score is", ps_ld_time, "seconds.\n")
cat("Processing time for runing pairing 5 is" ,pair5slow$running_time, "seconds.\n")
```

***

## Pairing 12

>Algorithm: `Doubly Robust Estimation`

>Distance Measure: `Propensity Score`

>Propensity Score Estimation: `Logistic Regression`

### Introduction of the algorithm

* The Doubly Robust Estimation has the smallest asymptotic variance. It remains consistent if the outcome models are wrong but the propensity model is right, or if the propensity model is wrong but the outcome models are right.

* Doubly Robust Estimator formula:
$$\hat{\Delta_{DR}} = N^{-1}\sum_{i=1}^{N} \frac{T_iY_i-(T_i-\hat{e_i}) \hat{m_1}(X_i)}{\hat{e_i}} - N^{-1}\sum_{i=1}^{N} \frac{(1-T_i)Y_i-(T_i-\hat{e_i}) \hat{m_0}(X_i)}{1-\hat{e_i}}$$

where $\hat{e_i}$ is the estimated propensity score for individual $i$, $\hat{m_t}(X)$ is a consistent estimate for $E(Y|T=t,X)$ and is usually obtained by regressing the observed response Y on X in group t.



```{r doubly robust estimation}
DoublyRobustEst <- function(data) {
  
  X = data %>% select(-Y, -A)
  n <- dim(data)[1]
  
  start_time1 <- Sys.time()
  #get propensity scores by using logistic regression
  logit_model <- glm(A ~., data=data[,-1], family="binomial")
  propensity <- predict(logit_model, X, type="response")
  data$ps <- propensity
  end_time1 <- Sys.time()
  
  start_time2 <- Sys.time()
  #split treatment and control group, and do regression for each group
  control <- data[data$A == 0, -2]
  treatment <- data[data$A == 1, -2]

  control_model <- lm(Y ~., data=control)
  treatment_model <- lm(Y ~., data=treatment)
  
  data$m0 <- predict(control_model, data[,-c(1,2)])
  data$m1 <- predict(treatment_model, data[,-c(1,2)])
  
  #calculate ATE
  ATE <- sum((data$A*data$Y-(data$A-data$ps)*data$m1)/data$ps)/n - 
         sum(((1-data$A)*data$Y+(data$A-data$ps)*data$m0)/(1-data$ps))/n
  
  end_time2 <- Sys.time()
  
  time1 <- round(end_time1 - start_time1,3)
  time2 <- round(end_time2 - start_time1,3)
  
  df <- data.frame(cbind(round(ATE,4), time1, time2))
  
  return(df)
}

```

#### highDim data

```{r highDim data for pairing 12}
DRE.high <- DoublyRobustEst(highDim_data)
diff.high <- true_high_ATE-DRE.high[,1]
```

```{r paring12 result highDim, echo=FALSE}
cat("ATE for high dimensional data is:", DRE.high[,1], ".\n")
cat("ATE error for high dimensional data is:", diff.high, ".\n")
cat("Processing time for calculating the propensity score is" ,DRE.high[,2], "seconds.\n")
cat("Processing time for running the algorithm is" ,DRE.high[,3], "seconds.\n")
``` 

#### lowDim data

```{r lowDim data for pairing 12}
DRE.low <- DoublyRobustEst(lowDim_data)
diff.low <- true_low_ATE-DRE.low[,1]
```

```{r paring 12 result lowDim, echo=FALSE}
cat("ATE for low dimensional data is:", DRE.low[,1], ".\n")
cat("ATE error for low dimensional data is:", diff.low, ".\n")
cat("Processing time for calculating the propensity score is" ,DRE.low[,2], "seconds.\n")
cat("Processing time for running the algorithm is" ,DRE.low[,3], "seconds.\n")
```

***

## Pairing 16

>Algorithm: `Stratification`

>Distance Measure: `Propensity Score`

>Propensity Score Estimation: `Logistic Regression`

### Introduction of the algorithm

* Stratification method definition:

$$\hat{\Delta_S}=\sum_{j=1}^{K} \frac{N_j}{N} \{N_{1j}^{-1}\sum_{i=1}{N}T_iY_iI(\hat{e_i}\in \hat{Q_j})-N_{0j}^{-1}\sum_{i=1}^{N}(1-T_i)Y_iI(\hat{e_i}\in \hat{Q_j})\}$$
Where K is the number of strata, $N_j$ is the number of individuals in stratum $j$, $N_{ij}$ is the number of treated individuals in stratum $j$, and $N_{0j}$ is the number of controlled individuals in stratum $j$.

Here we use K=7 as advised by the first article (Chan, Ge, Gershony, Hesterberg & Lambert).

#### Perform stratification and find difference of means and weights.
```{r}


n_strat <- 7
#High Dimensional Data
hd_prep <- system.time({
  
  #Stratify the Data by Propensity score
  hd_strat_divider <- n_strat/nrow(ps_hd_df);
  strat_hd_df <- ps_hd_df %>% arrange(prop_score) %>%
    mutate(stratum =  as.integer((1:n() -.5)*hd_strat_divider ) + 1);
  
  #Differences in means
  diff_mean_hd <- strat_hd_df %>%
    group_by(treatment, stratum) %>%
    summarise(mean_treat = mean(Y), .groups = "keep") %>%
    mutate(mean_treat= if_else(treatment == 0, -mean_treat, mean_treat)) %>%
    ungroup(treatment) %>%
    mutate(diff_mean = sum(mean_treat)) %>%
    filter(treatment == 0) %>%
    select(stratum, diff_mean);
  #Weights for each stratum
  diff_weight_hd <- strat_hd_df %>%
    group_by(stratum) %>%
    tally() %>%
    mutate(weight = n/nrow(strat_hd_df)) %>%
    left_join(diff_mean_hd, by = "stratum") %>%
    select(weight, diff_mean);
  
  #Calculate the ATE
  ATE_hd <- sum(diff_weight_hd$diff_mean * diff_weight_hd$weight)
})
time_hd <-sum(hd_prep)
#Low Dimensional Data
ld_prep <- system.time({
  
  #Stratify the Data by Propensity score
  ld_strat_divider <- n_strat/nrow(ps_ld_df);
  strat_ld_df <- ps_ld_df %>% arrange(prop_score) %>%
    mutate(stratum =  as.integer((1:n() -.5)*ld_strat_divider ) + 1);
  #Differences in means
  diff_mean_ld <- strat_ld_df %>%
    group_by(treatment, stratum) %>%
    summarise(mean_treat = mean(Y), .groups = "keep") %>%
    mutate(mean_treat= if_else(treatment == 0, -mean_treat, mean_treat)) %>%
    ungroup(treatment) %>%
    mutate(diff_mean = sum(mean_treat)) %>%
    filter(treatment == 0) %>%
    select(stratum, diff_mean);
  #Weights for each stratum
  diff_weight_ld <- strat_ld_df %>%
    group_by(stratum) %>%
    tally() %>%
    mutate(weight = n/nrow(strat_ld_df)) %>%
    left_join(diff_mean_ld, by = "stratum") %>%
    select(weight, diff_mean);
  
  #Calculate the ATE
  ATE_ld <- sum(diff_weight_ld$diff_mean * diff_weight_ld$weight)
}) 
time_ld <- sum(ld_prep)
```

#### Alternate Method that performs regression on each stratum by Y with A and Vs. Then averages the coefficients 

* alternate Method definition: 

$$\hat{\Delta^{(j)}}=n_j^{-1} \sum_{i=1}^{n}I(\hat{e_i}\in\hat{Q_j})\{m^{(j)}(1,X_i,\hat{\alpha^{(j)}})-m^{(j)}(0,X_i,\hat{\alpha^{(j)}})\}$$

* This is an alternative method for calculating the ATE once the rows are stratified by performing regression on each stratum for (Y ~ A + Covariates) and using the mean of the coefficients among strata as the ATE. 

* In larger numbers of strata, there is a risk of getting NAs for coefficients for the last few Covariates, if the number of observations in a stratum is less than the number of covariates + 1



### for A in each stratum to get the ATE

```{r alternative method}
n_strat <- 7
#High Dimensional Data
hd_prep_alt <- system.time({
  hd_strat_divider <- n_strat/nrow(ps_hd_df);
  
  #Append propensity score to the initial dataset
  hd_pscore <- matrix(ps_hd_df$prop_score, ncol = 1);
  hd_df_pscore <- cbind(highDim_data, hd_pscore);
  
  strat_hd_df_alt <- hd_df_pscore %>% arrange(hd_pscore) %>%
    mutate(stratum =  as.integer((1:n() -.5)*hd_strat_divider ) + 1) %>%
    select(-hd_pscore);
  
  strat_hd_reg <- strat_hd_df_alt %>% group_by(stratum) %>%
    do(strat_reg = lm(Y ~ A + .-stratum, data = .)) %>%
    mutate(coefs = list(strat_reg[["coefficients"]])) %>%
    select(-strat_reg) %>%
    summarize(ajz = coefs[["A"]], .groups = "drop");
 
  #Calculate the ATE
  ATE_hd_alt <-  mean(strat_hd_reg$ajz)
  
})
time_hd_alt <- sum(hd_prep_alt)
#Low Dimensional Data
ld_prep_alt <- system.time({
  
  ld_strat_divider <- n_strat/nrow(ps_ld_df);
  
  #Append propensity score to the initial dataset
  ld_pscore <- matrix(ps_ld_df$prop_score, ncol = 1);
  ld_df_pscore <- cbind(lowDim_data, ld_pscore);
  
  strat_ld_df_alt <- ld_df_pscore %>% arrange(ld_pscore) %>%
    mutate(stratum =  as.integer((1:n() -.5)*ld_strat_divider ) + 1) %>%
    select(-ld_pscore);
  
  strat_ld_reg <- strat_ld_df_alt %>% group_by(stratum) %>%
    do(strat_reg = lm(Y ~ A + .-stratum, data = .)) %>%
    mutate(coefs = list(strat_reg[["coefficients"]])) %>%
    select(-strat_reg) %>%
    summarize(ajz = coefs[["A"]], .groups = "drop")
  #Calculate the ATE
  ATE_ld_alt <-  mean(strat_ld_reg$ajz)
  
}) 
time_ld_alt <- sum(ld_prep_alt)
```


### Calculates the ATE and process time
#### High Dim data
```{r highDim for pairing 12, echo=FALSE}
cat("ATE for stratification is:", ATE_hd, ".\n")
cat("ATE error for stratification is:", true_high_ATE-ATE_hd, ".\n")
cat("Processing time for stratification is" , time_hd, "seconds.\n")
cat("\nATE for stratification alternative is:", ATE_hd_alt, ".\n")
cat("ATE error for stratification alternative is:", true_high_ATE-ATE_hd_alt, ".\n")
cat("Processing time for stratification is" , time_hd_alt, "seconds.\n")
cat("\nProcessing time for calculating the propensity score is", ps_hd_time, "seconds.\n")
```

#### Low Dim data
```{r lowDim for pairing 12, echo=FALSE}
cat("ATE for stratification is:", ATE_ld, ".\n")
cat("ATE error for stratification is:", true_low_ATE-ATE_ld, ".\n")
cat("Processing time for stratification is" , time_ld, "seconds.\n")
cat("\nATE for stratification alternative is:", ATE_ld_alt, ".\n")
cat("ATE error for stratification alternative is:", true_low_ATE-ATE_ld_alt, ".\n")
cat("Processing time for stratification is" , time_ld_alt, "seconds.\n")
cat("\nProcessing time for calculating the propensity score is", ps_ld_time, "seconds.\n")
```

## Summary

The table shows the ATEs, ATE errors, and times for all of our algorithms for both low dimensional and high dimensional datasets.

![](../figs/summary.png)

Based on our calculation on ATE and processing time for the five algorithms that we implemented, we conclude that Doubly Robust Estimation works best on low dimension dataset and the stratification with alternative method is the most efficient on high dimension dataset.


## Sources

Chan, David, Rong Ge, Ori Gershony, Tim Hesterberg, and Diane Lambert. 2010. “Evaluating Online Ad Campaigns in a Pipeline: Causal Models at Scale.” In Proceedings of the 16th Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, 7–16.

Lunceford, Jared K, and Marie Davidian. 2004. “Stratification and Weighting via the Propensity Score in Estimation of Causal Treatment Effects a Comparative Study.” Statistics in Medicine 23 (19): 2937–60.

P. Rosenbaum and D. B. Rubin. Reducing bias in observational studies using subclassification on the propensity score. Journal of the American Statistical Association, 79:516–524, 1984.

Stuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 25 (1): 1.

